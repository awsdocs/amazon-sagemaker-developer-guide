# Deploy a Model<a name="deploy-model"></a>

For an overview on deploying a model with Amazon SageMaker, see [Deploy a Model in Amazon SageMaker](how-it-works-deployment.md)\.

Amazon SageMaker provides features to manage resources and optimize inference performance when deploying machine learning models\. For guidance on using inference pipelines, Neo, Elastic inference, automatic scaling, inference hosting instances, and on best practices, see the following topics\.
+ For guidance using Amazon SageMaker inference pipeline to manage data processing and real\-time predictions or process batch transforms in a series of Docker containers: , see [Amazon SageMaker Inference Pipelines](inference-pipelines.md)\.
+ For guidance using Amazon SageMaker Neo to enable machine learning models to train once and run anywhere in the cloud, see [Amazon SageMaker Neo](neo.md)\.
+ For guidance using Elastic Inference \(EI\) to speed up the throughput and decrease the latency of getting real\-time inferences from your deep learning models that are deployed as Amazon SageMaker hosted models using a GPU instance for your endpoint, see [Amazon SageMaker Elastic Inference \(EI\) ](ei.md)\.
+ For guidance on using automatic scaling to dynamically adjust the number of instances provisioned for a production variant in response to changes in your workload, see [Automatically Scale Amazon SageMaker Models](endpoint-auto-scaling.md)\.
+ For information about the size of storage volumes on different sizes of hosting instances, see [Hosting Instance Storage Volumes](host-instance-storage.md)\.
+ For guidance on best practices to use for model deployment, see [Best Practices for Deploying Amazon SageMaker Models](best-pratices.md)\.