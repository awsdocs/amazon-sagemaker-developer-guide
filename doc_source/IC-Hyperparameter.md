# Hyperparameters<a name="IC-Hyperparameter"></a>


| Parameter Name | Description | 
| --- | --- | 
| num\_classes | Number of output classes\. This parameter defines the dimensions of the network output and is typically set to the number of classes in the dataset\. Valid values: positive integer Default value: \-  | 
| num\_training\_samples | Number of training examples in the input dataset\. If there is a mismatch between this value and the number of samples in the training set, then the behavior of the `lr_scheduler_step` parameter is undefined and distributed training accuracy might be affected\. Valid values: positive integer Default value: \-  | 
| use\_pretrained\_model | Flag to indicate whether to use pre\-trained model for training\. If set to 1, then the pretrained model with the corresponding number of layers is loaded and used for training\. Only the top FC layer are reinitialized with random weights\. Otherwise, the network is trained from scratch\. Valid values: 0 or 1 Default value: 0 | 
| checkpoint\_frequency | Period to store model parameters \(in number of epochs\)\. Valid values: positive integer no greater than `epochs`\. Default value: `epochs` \(save checkpoint at the last epoch\) | 
| num\_layers | Number of layers for the network\. For data with large image size \(for example, 224x224 \- like ImageNet\), we suggest selecting the number of layers from the set \[18, 34, 50, 101, 152, 200\]\. For data with small image size \(for example, 28x28 \- like CIFAR\), we suggest selecting the number of layers from the set \[20, 32, 44, 56, 110\]\. The number of layers in each set is based on the ResNet paper\. For transfer learning, the number of layers defines the architecture of base network and hence can only be selected from the set \[18, 34, 50, 101, 152, 200\]\. Valid values: Positive integer in \[18, 34, 50, 101, 152, 200\] or \[20, 32, 44, 56, 110\]\. Default value: 152 | 
| resize | Resize the image before using it for training\. The images are resized so that the shortest side is of this parameter\. If the parameter is not set, then the training data is used as such without resizing\. Note: This option is available only for inputs specified as `application/x-image` content\-type in training and validation channels\. Valid values: positive integer Default value: \-  | 
| epochs | Number of training epochs\. Valid values: positive integer Default value: 30 | 
| learning\_rate | Initial learning rate\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.1 | 
| lr\_scheduler\_factor | The ratio to reduce learning rate used in conjunction with the `lr_scheduler_step` parameter, defined as `lr_new` = `lr_old` \* `lr_scheduler_factor`\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.1 | 
| lr\_scheduler\_step | The epochs at which to reduce the learning rate\. As explained in the `lr_scheduler_factor` parameter, the learning rate is reduced by `lr_scheduler_factor` at these epochs\. For example, if the value is set to "10, 20", then the learning rate is reduced by `lr_scheduler_factor` after 10th epoch and again by `lr_scheduler_factor` after 20th epoch\. The epochs are delimited by ","\. Valid values: string Default value: \- | 
| optimizer | The optimizer types\. For more details of the parameters for the optimizers, please refer to MXNet's API\. Valid values: One of *sgd*, *adam*, *rmsprop*, or *nag*\. Default value: *sgd* | 
| momentum | The momentum for *sgd* and *nag*, ignored for other optimizers\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0 | 
| weight\_decay | The coefficient weight decay for *sgd* and *nag*, ignored for other optimizers\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.0001 | 
| beta\_1 | The beta1 for *adam*, in other words, exponential decay rate for the first moment estimates\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.9 | 
| beta\_2 | The beta2 for *adam*, in other words, exponential decay rate for the second moment estimates\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.999 | 
| eps | The epsilon for *adam* and *rmsprop*\. It is usually set to a small value to avoid division by 0\. Valid values: Float\. Range in \[0, 1\]\. Default value: 1e\-8 | 
| gamma | The gamma for *rmsprop*\. A decay factor of moving average of the squared gradient\. Valid values: Float\. Range in \[0, 1\]\. Default value: 0\.9 | 
| mini\_batch\_size | The batch size for training\. In a single\-machine multi\-GPU setting, each GPU handles `mini_batch_size`/num\_gpu training samples\. For the multi\-machine training in dist\_sync mode, the actual batch size is `mini_batch_size`\*number of machines\. See MXNet docs for more details\. Valid values: positive integer Default value: 32 | 
| image\_shape | The input image dimensions, which is the same size as the input layer of the network\. The format is defined as '`num_channels`, height, width'\. The image dimension can take on any value as the network can handle varied dimensions of the input\. However, there may be memory constraints if a larger image dimension is used\. Typical image dimensions for image classification are '3, 224, 224'\. This is similar to the ImageNet dataset\. Valid values: string Default value: ‘3, 224, 224’ | 
| augmentation\_type |  Data augmentation type\. The input images can be augmented in multiple ways as specified below\. [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html) Valid values: One of `crop`, `crop_color`, `crop_color_transform`\. Default value: \-  | 
| top\_k | Report the top\-k accuracy during training\. This parameter has to be greater than 1, since the top\-1 training accuracy is the same as the regular training accuracy that has already been reported\. Valid values: Positive integer larger than 1\. Default value: \- | 
| kv\_store |  Weight update synchronization mode during distributed training\. The weight updates can be updated either synchronously or asynchronously across machines\. Synchronous updates typically provide better accuracy than asynchronous updates but can be slower\. See distributed training in MXNet for more details\. This parameter is not applicable to single machine training\. [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html) Valid values: Either `dist_sync` or `dist_async`\. Default value: none  | 