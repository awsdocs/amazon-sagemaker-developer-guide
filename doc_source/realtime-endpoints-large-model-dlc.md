# Deep learning containers for large model inference<a name="realtime-endpoints-large-model-dlc"></a>

 SageMaker maintains deep learning containers \(DLCs\) with popular open source libraries for hosting large models such as GPT, OPT, BLOOM, and Stable Diffusion on AWS infrastructure\. With these DLCs you can use third party libraries such as [DeepSpeed](https://www.deepspeed.ai/) and [Accelerate](https://huggingface.co/docs/accelerate/index) to partition model parameters using model parallelism techniques to leverage the memory of multiple GPUs for inference\. The following table lists the DLCs available with SageMaker for LMI\. We recommend that you start with these DLCs for LMI on SageMaker\. These include components, libraries, and drivers that have been optimized and tested for use on SageMaker\. 

[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)

 In addition to PyTorch, LMI DLCs include libraries to facilitate LMI\. SageMaker supports the following categories of libraries\. 
+  **Model zoo** – Model zoos provide simple API access to pre\-trained models\. SageMaker provides the following model zoos: 
  +  [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) is a popular library for pre\-trained deep learning models that use a transformer architecture such as GPT, OPT, and BLOOM\. 
  +  [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index) is a library with pre\-trained deep learning models that use a diffusion technique such as Stable Diffusion\. 
+  **Model parallelism and inference optimization libraries** – These libraries handle model parallel inference by partitioning a model artifact so that its comprising parameters can be spread across multiple GPUs\. SageMaker supports the following model parallelism and inference optimization libraries: 
  +  [DeepSpeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) is an open source inference optimization library\. It includes model partitioning schemes that enable model parallelism with supported models, including many transformer models\. It also has optimized kernels for popular models such as OPT, GPT, and BLOOM that can significantly improve inference latency\. The version of DeepSpeed in the LMI DLCs is optimized and tested to work on SageMaker\. It includes several enhancements, including support for BF16 precision models\. 
  +  [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/index) in an open source model parallel inference library\. It supports model parallelism for most models in the Hugging Face Transformers library\. 
+  **Model server** – Model servers handle an inference request end to end\. They accept requests, invoke pre\-processing and post\-processing scripts, and respond to users\. Model servers that are compatible with model parallelism, also organize workers and threads across multiple devices\. SageMaker supports the following model servers: 
  +  [DJL\-Serving](https://github.com/deepjavalibrary/djl-serving) is an open source, high performance model server powered by [DJL](https://djl.ai/)\. It takes multiple deep learning models or workflows, and makes them available through an HTTP endpoint\. Versions 0\.19 and above are supported by SageMaker and work with Amazon EC2 instances with multiple GPUs to facilitate LMI with model parallelism\. 

## Supported instance types<a name="realtime-endpoints-large-model-dlc-instance"></a>

 AWS LMI DLCs support the `p4d`, `p3`, `g5`, and `g4dn` instance types\. 